## **üìú Descri√ß√£o do Projeto**

Este projeto demonstra uma arquitetura de **streaming de dados em tempo real** utilizando **Kafka**, **PySpark (Structured Streaming)** e **Delta Lake**.  
Ele simula dispositivos IoT que enviam dados para um t√≥pico Kafka, que s√£o consumidos e processados em tempo real pelo PySpark e armazenados no formato Delta Lake.


## **üîé Fluxo de Dados**

1. **IoT Simulator**  
   - Gera dados fake usando a biblioteca **Faker**.  
   - Cria uma Api que simula o funcionamento de um iot.

2. **Kafka**  
   - Atua como um **message broker**, recebendo os dados do simulador e garantindo a entrega para o consumidor.  
   - Mant√©m os dados particionados e replicados para garantir toler√¢ncia a falhas.

3. **PySpark Structured Streaming**  
   - Consome dados diretamente do t√≥pico Kafka.  
   - Faz o parsing do JSON recebido.  
   - Enriquecimento e padroniza√ß√£o dos dados (ex.: truncar a data para granularidade hor√°ria).  
   - Persiste os dados no **Delta Lake** (camada *bronze*).


4. **Delta Lake**  
   - Tabela transacional armazenada em `/data/delta/bronze/iot_emulado`.  
   - Suporte a versionamento, esquema evolutivo e alta confiabilidade.


## **üõ†Ô∏è Tecnologias Utilizadas**

- **Apache Kafka** ‚Äì Mensageria e ingest√£o de dados em tempo real.
- **Apache Spark (PySpark)** ‚Äì Processamento distribu√≠do em streaming.
- **Delta Lake** ‚Äì Armazenamento confi√°vel com suporte ACID.
- **Docker Compose** ‚Äì Orquestra√ß√£o dos containers (Kafka, Zookeeper, Spark, etc.).
- **Python (Faker)** ‚Äì Gera√ß√£o de dados IoT simulados.

## **‚ö° Execu√ß√£o do Projeto**

Executar o processo por completo:
    
    ```PowerShell
    docker compose up -d
    ```

## **Validar resultados**

    ```PowerShell
    docker exec -it pyspark-consumer spark-submit --packages io.delta:delta-core_2.12:2.4.0 /app/spark/consultando_dados.py
    ```
