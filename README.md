## PROJETO 1

# ğŸ§­ DiÃ¡rio de Bordo - Pipeline de Dados com PySpark e Delta Lake

Este projeto implementa um pipeline de processamento de dados utilizando **Apache Spark**, com foco em dados de deslocamento (categoria, distÃ¢ncia, propÃ³sito, etc.), permitindo o agrupamento, validaÃ§Ã£o e tratamento de dados invÃ¡lidos em uma arquitetura moderna com **Delta Lake**.

## ğŸ”„ Fluxo de Processamento

1. **Leitura da Camada Bronze**
   - Os dados brutos sÃ£o carregados a partir de arquivos CSV com formataÃ§Ã£o de datas personalizada.

2. **ValidaÃ§Ã£o dos Dados**
   - As linhas com colunas nulas (ex: `CATEGORIA`, `PROPOSITO`, `DISTANCIA`, `DATA_INICIO`) sÃ£o filtradas.
   - Dados invÃ¡lidos sÃ£o armazenados em uma **tabela de quarentena** para anÃ¡lise posterior.

3. **TransformaÃ§Ãµes**
   - As datas sÃ£o convertidas para o padrÃ£o `yyyy-MM-dd`.
   - AgregaÃ§Ãµes como mÃ©dia, mÃ¡ximo, mÃ­nimo e contagens por categoria e propÃ³sito sÃ£o aplicadas.

4. **GravaÃ§Ã£o na Camada Silver**
   - Os dados transformados e agregados sÃ£o salvos em formato Delta, particionados por data (`dt_refe`).

## ğŸ“ Estrutura de Pastas

``` bash
diario_de_bordo/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ DiarioDeBordo/
â”‚ â”‚ â”œâ”€â”€ DiarioDeBordoExecute.py
â”‚ â”‚ â”œâ”€â”€ DiarioDeBordoTransform.py
â”‚ â”‚ â”œâ”€â”€ DiarioDeBordoEnum.py
â”‚ â”‚ â”œâ”€â”€ DiarioDeBordoCreateTable.py
â”‚ â”‚ â””â”€â”€ LoggerService.py
â”œâ”€â”€ data/
â”‚ â””â”€â”€ bronze/
â””â”€â”€ README.md
```

## ğŸ› ï¸ Tecnologias Utilizadas

- Apache Spark
- Delta Lake
- PySpark
- Databricks
- Python 3.10

## ğŸš¨ Tratamento de Erros

- Logs sÃ£o gerenciados com o mÃ³dulo `logging`, com exibiÃ§Ã£o no console.
- Estrutura pronta para integraÃ§Ã£o com tabelas de erro se necessÃ¡rio.

## ğŸ“Œ Objetivo

Garantir o controle e a rastreabilidade de dados de transporte, permitindo anÃ¡lises confiÃ¡veis mesmo em ambientes com dados incompletos ou inconsistentes.

Notebook: https://dbc-35f07cc0-9f86.cloud.databricks.com/login.html?next_url=%2Feditor%2Fnotebooks%2F3406324339502378%3Fo%3D2187747513666326&tuuid=89c103bb-0583-466f-9b17-9f0d8efcd0a2#command/6501943768248975

github: https://github.com/Enricolm/diario_de_bordo/tree/main


## PROJETO 2
## **ğŸ“œ DescriÃ§Ã£o do Projeto**

Este projeto demonstra uma arquitetura de **streaming de dados em tempo real** utilizando **Kafka**, **PySpark (Structured Streaming)** e **Delta Lake**.  
Ele simula dispositivos IoT que enviam dados para um tÃ³pico Kafka, que sÃ£o consumidos e processados em tempo real pelo PySpark e armazenados no formato Delta Lake.


## **ğŸ” Fluxo de Dados**

1. **IoT Simulator**  
   - Gera dados fake usando a biblioteca **Faker**.  
   - Cria uma Api que simula o funcionamento de um iot.

2. **Kafka**  
   - Atua como um **message broker**, recebendo os dados do simulador e garantindo a entrega para o consumidor.  
   - MantÃ©m os dados particionados e replicados para garantir tolerÃ¢ncia a falhas.

3. **PySpark Structured Streaming**  
   - Consome dados diretamente do tÃ³pico Kafka.  
   - Faz o parsing do JSON recebido.  
   - Enriquecimento e padronizaÃ§Ã£o dos dados (ex.: truncar a data para granularidade horÃ¡ria).  
   - Persiste os dados no **Delta Lake** (camada *bronze*).


4. **Delta Lake**  
   - Tabela transacional armazenada em `/data/delta/bronze/iot_emulado`.  
   - Suporte a versionamento, esquema evolutivo e alta confiabilidade.


## **ğŸ› ï¸ Tecnologias Utilizadas**

- **Apache Kafka** â€“ Mensageria e ingestÃ£o de dados em tempo real.
- **Apache Spark (PySpark)** â€“ Processamento distribuÃ­do em streaming.
- **Delta Lake** â€“ Armazenamento confiÃ¡vel com suporte ACID.
- **Docker Compose** â€“ OrquestraÃ§Ã£o dos containers (Kafka, Zookeeper, Spark).
- **Python (Faker)** â€“ GeraÃ§Ã£o de dados IoT simulados.

## Estrutura de pasta

```bash
IOT_FAKER_KAFKA_SPARK_DELTA/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ kafka/                   
â”‚   â”‚   â”œâ”€â”€ error/
â”‚   â”‚   â””â”€â”€ producer.py
â”‚   â”œâ”€â”€ service/                  
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ Exception.py
â”‚   â”‚   â””â”€â”€ LoggerService.py
â”‚   â”œâ”€â”€ spark/                    
â”‚   â”‚   â”œâ”€â”€ error/
â”‚   â”‚   â”œâ”€â”€ consultando_dados.py
â”‚   â”‚   â”œâ”€â”€ consumer.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ delta/
â”‚   â””â”€â”€ bronze/
â”‚       â””â”€â”€ iot_emulado/         
â”œâ”€â”€ iot_simulator/               
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```


## **âš¡ ExecuÃ§Ã£o do Projeto**

Executar o processo por completo:
    
    
    docker compose up --build -d
    

## **ğŸ‘¤ Autor**

**Enrico Lopes Malachini**  
ğŸ“§ **E-mail:** [rico.malachini@gmail.com](mailto:rico.malachini@gmail.com)  
ğŸ”— **GitHub:** [github.com/Enricolm](https://github.com/Enricolm)  
ğŸ”— **LinkedIn:** [linkedin.com/in/enrico-malachini](https://www.linkedin.com/in/enrico-malachini)

